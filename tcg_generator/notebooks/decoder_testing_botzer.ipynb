{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Testing of a Decoder-only (GPT style) architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "\n",
    "The training of the tokenizer and other items was done on the FULL dataset.  This causes a data leak and proper train/val/test splitting should be done in the final product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tokenizers\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import Dataset, load_dataset #, load_metric\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import ast # Because the tokenized_text column data is stored as a string instead of a list...\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Check tensforflow GPU availability\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# It looks like Brandon's rig hates him right now... stupid WSL2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Get the dataset\n",
    "2) Fix the tokenize_text column from a string to a list of strings\n",
    "3) Pull out all of the words into a mega-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the processed card data from wherever you have it.\n",
    "\n",
    "# df = pd.read_csv('../data/processed/mtg_carddata_processed.csv')\n",
    "df = pd.read_csv('..\\data\\processed\\mtg_carddata_processed_2_23_25.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mana_cost</th>\n",
       "      <th>type_line</th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>colors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mtgo_id</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>defense</th>\n",
       "      <th>processed_oracle_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tfidf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nissa, Worldsoul Speaker</td>\n",
       "      <td>{3}{G}</td>\n",
       "      <td>Legendary Creature — Elf Druid</td>\n",
       "      <td>Landfall — Whenever a land you control enters,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Landfall']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Landfall — Whenever a land you control enters ...</td>\n",
       "      <td>['Landfall', 'Whenever', 'a', 'land', 'you', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Static Orb</td>\n",
       "      <td>{3}</td>\n",
       "      <td>Artifact</td>\n",
       "      <td>As long as &lt;name&gt; is untapped, players can't u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>15870.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As long as Static Orb is untapped , players ca...</td>\n",
       "      <td>['As', 'long', 'as', '&lt;name&gt;', 'is', 'untapped...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensory Deprivation</td>\n",
       "      <td>{U}</td>\n",
       "      <td>Enchantment — Aura</td>\n",
       "      <td>Enchant creature\\r\\nEnchanted creature gets -3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Enchant']</td>\n",
       "      <td>49283.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enchant creature \\n Enchanted creature gets -3...</td>\n",
       "      <td>['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Road of Return</td>\n",
       "      <td>{G}{G}</td>\n",
       "      <td>Sorcery</td>\n",
       "      <td>Choose one —\\r\\n• Return target permanent card...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Entwine']</td>\n",
       "      <td>77122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Choose one — \\n • Return target permanent card...</td>\n",
       "      <td>['Choose', 'one', '\\\\n', 'Return', 'target', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Storm Crow</td>\n",
       "      <td>{1}{U}</td>\n",
       "      <td>Creature — Bird</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Flying']</td>\n",
       "      <td>22609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>['Flying', 'This', 'creature', \"can't\", 'be', ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30618</th>\n",
       "      <td>Devoted Hero</td>\n",
       "      <td>{W}</td>\n",
       "      <td>Creature — Elf Soldier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30619</th>\n",
       "      <td>Without Weakness</td>\n",
       "      <td>{1}{B}</td>\n",
       "      <td>Instant</td>\n",
       "      <td>Target creature you control gains indestructib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['Cycling']</td>\n",
       "      <td>64646.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Target creature you control gains indestructib...</td>\n",
       "      <td>['Target', 'creature', 'you', 'control', 'gain...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30620</th>\n",
       "      <td>Firesong and Sunspeaker</td>\n",
       "      <td>{4}{R}{W}</td>\n",
       "      <td>Legendary Creature — Minotaur Cleric</td>\n",
       "      <td>Red instant and sorcery spells you control hav...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>['R', 'W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>101914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Red instant and sorcery spells you control hav...</td>\n",
       "      <td>['Red', 'instant', 'and', 'sorcery', 'spells',...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30621</th>\n",
       "      <td>Samut, the Tested</td>\n",
       "      <td>{2}{R}{G}</td>\n",
       "      <td>Legendary Planeswalker — Samut</td>\n",
       "      <td>+1: Up to one target creature gains double str...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G', 'R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>64772.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+1 : Up to one target creature gains double st...</td>\n",
       "      <td>['1', ':', 'Up', 'to', 'one', 'target', 'creat...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30622</th>\n",
       "      <td>Sinew Sliver</td>\n",
       "      <td>{1}{W}</td>\n",
       "      <td>Creature — Sliver</td>\n",
       "      <td>All Sliver creatures get +1/+1.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>114015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Sliver creatures get +1/+1 .</td>\n",
       "      <td>['All', 'Sliver', 'creatures', 'get', '1', '1'...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30623 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  mana_cost  \\\n",
       "0      Nissa, Worldsoul Speaker     {3}{G}   \n",
       "1                    Static Orb        {3}   \n",
       "2           Sensory Deprivation        {U}   \n",
       "3                Road of Return     {G}{G}   \n",
       "4                    Storm Crow     {1}{U}   \n",
       "...                         ...        ...   \n",
       "30618              Devoted Hero        {W}   \n",
       "30619          Without Weakness     {1}{B}   \n",
       "30620   Firesong and Sunspeaker  {4}{R}{W}   \n",
       "30621         Samut, the Tested  {2}{R}{G}   \n",
       "30622              Sinew Sliver     {1}{W}   \n",
       "\n",
       "                                  type_line  \\\n",
       "0            Legendary Creature — Elf Druid   \n",
       "1                                  Artifact   \n",
       "2                        Enchantment — Aura   \n",
       "3                                   Sorcery   \n",
       "4                           Creature — Bird   \n",
       "...                                     ...   \n",
       "30618                Creature — Elf Soldier   \n",
       "30619                               Instant   \n",
       "30620  Legendary Creature — Minotaur Cleric   \n",
       "30621        Legendary Planeswalker — Samut   \n",
       "30622                     Creature — Sliver   \n",
       "\n",
       "                                             oracle_text power toughness  \\\n",
       "0      Landfall — Whenever a land you control enters,...     3         3   \n",
       "1      As long as <name> is untapped, players can't u...   NaN       NaN   \n",
       "2      Enchant creature\\r\\nEnchanted creature gets -3...   NaN       NaN   \n",
       "3      Choose one —\\r\\n• Return target permanent card...   NaN       NaN   \n",
       "4      Flying (This creature can't be blocked except ...     1         2   \n",
       "...                                                  ...   ...       ...   \n",
       "30618                                                NaN     1         2   \n",
       "30619  Target creature you control gains indestructib...   NaN       NaN   \n",
       "30620  Red instant and sorcery spells you control hav...     4         6   \n",
       "30621  +1: Up to one target creature gains double str...   NaN       NaN   \n",
       "30622                    All Sliver creatures get +1/+1.     1         1   \n",
       "\n",
       "           colors      keywords   mtgo_id loyalty  defense  \\\n",
       "0           ['G']  ['Landfall']       NaN     NaN      NaN   \n",
       "1              []            []   15870.0     NaN      NaN   \n",
       "2           ['U']   ['Enchant']   49283.0     NaN      NaN   \n",
       "3           ['G']   ['Entwine']   77122.0     NaN      NaN   \n",
       "4           ['U']    ['Flying']   22609.0     NaN      NaN   \n",
       "...           ...           ...       ...     ...      ...   \n",
       "30618       ['W']            []       NaN     NaN      NaN   \n",
       "30619       ['B']   ['Cycling']   64646.0     NaN      NaN   \n",
       "30620  ['R', 'W']            []  101914.0     NaN      NaN   \n",
       "30621  ['G', 'R']            []   64772.0       4      NaN   \n",
       "30622       ['W']            []  114015.0     NaN      NaN   \n",
       "\n",
       "                                   processed_oracle_text  \\\n",
       "0      Landfall — Whenever a land you control enters ...   \n",
       "1      As long as Static Orb is untapped , players ca...   \n",
       "2      Enchant creature \\n Enchanted creature gets -3...   \n",
       "3      Choose one — \\n • Return target permanent card...   \n",
       "4      Flying (This creature can't be blocked except ...   \n",
       "...                                                  ...   \n",
       "30618                                                NaN   \n",
       "30619  Target creature you control gains indestructib...   \n",
       "30620  Red instant and sorcery spells you control hav...   \n",
       "30621  +1 : Up to one target creature gains double st...   \n",
       "30622                   All Sliver creatures get +1/+1 .   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "0      ['Landfall', 'Whenever', 'a', 'land', 'you', '...   \n",
       "1      ['As', 'long', 'as', '<name>', 'is', 'untapped...   \n",
       "2      ['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...   \n",
       "3      ['Choose', 'one', '\\\\n', 'Return', 'target', '...   \n",
       "4      ['Flying', 'This', 'creature', \"can't\", 'be', ...   \n",
       "...                                                  ...   \n",
       "30618                                                 []   \n",
       "30619  ['Target', 'creature', 'you', 'control', 'gain...   \n",
       "30620  ['Red', 'instant', 'and', 'sorcery', 'spells',...   \n",
       "30621  ['1', ':', 'Up', 'to', 'one', 'target', 'creat...   \n",
       "30622  ['All', 'Sliver', 'creatures', 'get', '1', '1'...   \n",
       "\n",
       "                  tfidf_vector  \n",
       "0      [0. 0. 0. ... 0. 0. 0.]  \n",
       "1      [0. 0. 0. ... 0. 0. 0.]  \n",
       "2      [0. 0. 0. ... 0. 0. 0.]  \n",
       "3      [0. 0. 0. ... 0. 0. 0.]  \n",
       "4      [0. 0. 0. ... 0. 0. 0.]  \n",
       "...                        ...  \n",
       "30618  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30619  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30620  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30621  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30622  [0. 0. 0. ... 0. 0. 0.]  \n",
       "\n",
       "[30623 rows x 14 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Landfall', 'Whenever', 'a', 'land', 'you', 'control', 'enters', ',', 'you', 'get', '{E}', '{E}', 'two', 'energy', 'counters', '.', '\\\\\\\\n', 'You', 'may', 'pay', 'eight', '{E}', 'rather', 'than', 'pay', 'the', 'mana', 'cost', 'for', 'permanent', 'spells', 'you', 'cast', '.']\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Landfall, Whenever, a, land, you, control, en...\n",
       "1    [As, long, as, <name>, is, untapped, ,, player...\n",
       "2    [Enchant, creature, \\n, Enchanted, creature, g...\n",
       "3    [Choose, one, \\n, Return, target, permanent, c...\n",
       "4    [Flying, This, creature, can't, be, blocked, e...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Apply ast.literal_eval to each row in the tokenized_text column\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(ast.literal_eval)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df['tokenized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landfall'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 968000\n",
      "First 20 tokens: ['Landfall', 'Whenever', 'a', 'land', 'you', 'control', 'enters', ',', 'you', 'get', '{E}', '{E}', 'two', 'energy', 'counters', '.', '\\\\n', 'You', 'may', 'pay']\n"
     ]
    }
   ],
   "source": [
    "# Combine all tokens into one large list\n",
    "all_tokens = []\n",
    "for tokens in df['tokenized_text']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Alternative one-liner using list comprehension\n",
    "# all_tokens = [token for tokens in df['tokenized_text'] for token in tokens]\n",
    "\n",
    "# Display the first 20 tokens to verify\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(\"First 20 tokens:\", all_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Landfall — Whenever a land you control enters ...\n",
       "1        As long as Static Orb is untapped , players ca...\n",
       "2        Enchant creature \\n Enchanted creature gets -3...\n",
       "3        Choose one — \\n • Return target permanent card...\n",
       "4        Flying (This creature can't be blocked except ...\n",
       "                               ...                        \n",
       "30618                                                  NaN\n",
       "30619    Target creature you control gains indestructib...\n",
       "30620    Red instant and sorcery spells you control hav...\n",
       "30621    +1 : Up to one target creature gains double st...\n",
       "30622                     All Sliver creatures get +1/+1 .\n",
       "Name: processed_oracle_text, Length: 30623, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['processed_oracle_text']\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus to list of strings if it's not already\n",
    "corpus_list = corpus.tolist() if hasattr(corpus, 'tolist') else list(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Landfall — Whenever a land you control enters ...\n",
      "1    As long as Static Orb is untapped , players ca...\n",
      "2    Enchant creature \\n Enchanted creature gets -3...\n",
      "Name: processed_oracle_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas Series to list of strings\n",
    "corpus_list = corpus.values.tolist()\n",
    "\n",
    "# Ensure all elements are strings\n",
    "corpus_list = [str(text) for text in corpus_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landfall — Whenever a land you control enters , you get {E} {E} (two energy counters) . \\\\n You may pay eight {E} rather than pay the mana cost for permanent spells you cast .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_list))  # Should show: <class 'list'>\n",
    "print(type(corpus_list[0]))  # Should show: <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/val/test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need to keep other information with the corpus_list as we split it up?  Names, color, etc.?\n",
    "\n",
    "If we're just generating text, I think that answer is no.  If we want more, then the answer is likely yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(corpus_list, test_size=0.2) # Set 80% for training\n",
    "val, test = train_test_split(temp, test_size=0.5) # Set 10% for validation and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{3} : Hypnotic Grifter connives . (Draw a card , then discard a card . If you discarded a nonland card , put a +1/+1 counter on this creature . )',\n",
       " 'Burning Hands deals 2 damage to target creature or planeswalker . If that permanent is green , Burning Hands deals 6 damage instead .',\n",
       " '(Theme color : {U} )',\n",
       " \"Enchant creature \\\\n Enchanted creature gets +1/+1 and has lifelink . \\\\n When Squire's Devotion enters , create a 1/1 white Vampire creature token with lifelink .\",\n",
       " 'Devoid (This card has no color . ) \\\\n Flying \\\\n When Eldrazi Skyspawner enters , create a 1/1 colorless Eldrazi Scion creature token . It has \"Sacrifice this creature : Add {C} . \"']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Tokenizer on Training (or the full thing?) Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for tokenizer files if it doesn't exist\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Initialize trainer with specific output directory\n",
    "tk_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=8192,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "# Initialize tokenizer with output directory\n",
    "tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\")\n",
    "tokenizer.enable_truncation(max_length=512) # Maybe we cut this down to 258?\n",
    "\n",
    "# Train the tokenizer on just the test set of data to prevent data leakage\n",
    "# tokenizer.train_from_iterator(corpus_list, trainer=tk_trainer)\n",
    "tokenizer.train_from_iterator(train, trainer=tk_trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(os.path.join(models_dir, \"tokenizer.json\")) # Your tokenizer may have been trained with the full dataset causing a data leak..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: ['If', 'Leyline', 'of', 'Ab', 'und', 'ance', 'is', 'in', 'your', 'opening', 'hand', ',', 'you', 'may', 'begin', 'the', 'game', 'with', 'it', 'on', 'the', 'battlefield', '.', '\\\\', 'n', 'Whenever', 'you', 'tap', 'a', 'creature', 'for', 'mana', ',', 'add', 'an', 'additional', '{', 'G', '}', '.', '\\\\', 'n', '{', '6', '}', '{', 'G', '}', '{', 'G', '}', ':', 'Put', 'a', '+', '1', '/+', '1', 'counter', 'on', 'each', 'creature', 'you', 'control', '.']\n"
     ]
    }
   ],
   "source": [
    "# Verify the tokenizer works\n",
    "sample_text = val[0]\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "print(f\"Encoded: {encoded.tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'of', '\\\\', 't', 'non', 'z', 'ens', 'ic', 'all', 'ical', 'ness', '{', 'U', '}']\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer on a string with a tab character and non-sensical text\n",
    "test = tokenizer.encode(\"This is a test of \\\\t nonzensicallicalness {U}\")\n",
    "print(test.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since you have a pre-trained tokenizer, you can now load it directly\n",
    "tokenizer = Tokenizer.from_file(os.path.join(models_dir, \"tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embed_dims, seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size + 1, embed_dims)\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(seq_len, embed_dims, mask_zero=True)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        index_range = tf.shape(x)[-1]\n",
    "        y = self.embedding(x)\n",
    "        indices = tf.range(index_range)\n",
    "        pos = self.pos_embedding(indices)\n",
    "        return y + pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super().__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        y = self.attention(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x,\n",
    "            use_causal_mask=True)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8912 # Define this based on training data vocab size / tokenize size.\n",
    "# TODO: Or does it need to relate to the tokenizer?\n",
    "SEQUENCE_LENGTH = 500 # Define this based on training data sequence length.\n",
    "EMBEDDING_DIM = 512 # Configure\n",
    "FEED_FORWARD_DIM = 4 * EMBEDDING_DIM # This is usual value used by other LLMs, from below reference.\n",
    "DROPOUT = 0.5 # Configure\n",
    "NUM_HEADS = 16 # Configure\n",
    "NUM_DECODER_LAYERS = 4 # Configure\n",
    "\n",
    "### Reference: https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse\n",
    "\n",
    "decoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int64, name=\"decoder_inputs\")\n",
    "positional_embedding = PositionalEmbedding(\n",
    "    VOCAB_SIZE, EMBEDDING_DIM, SEQUENCE_LENGTH)(decoder_inputs)\n",
    "\n",
    "layer_input = positional_embedding\n",
    "layer_output = None\n",
    "for i in range(NUM_DECODER_LAYERS):\n",
    "    decoder_norm_1 = tf.keras.layers.LayerNormalization()(layer_input)\n",
    "    decoder_self_attention = DecoderSelfAttention(NUM_HEADS, EMBEDDING_DIM)(decoder_norm_1)\n",
    "    decoder_add_1 = tf.keras.layers.Add()([decoder_self_attention, decoder_norm_1])\n",
    "    decoder_norm_2 = tf.keras.layers.LayerNormalization()(decoder_add_1)\n",
    "    decoder_feedforward_1 = tf.keras.layers.Dense(FEED_FORWARD_DIM, activation=\"gelu\")(decoder_norm_2)\n",
    "    decoder_feedforward_2 = tf.keras.layers.Dense(EMBEDDING_DIM)(decoder_feedforward_1)\n",
    "    decoder_dropout = tf.keras.layers.Dropout(DROPOUT)(decoder_feedforward_2)\n",
    "    decoder_add_2 = tf.keras.layers.Add()([decoder_dropout, decoder_add_1])\n",
    "\n",
    "    layer_input = decoder_add_2\n",
    "    layer_output = decoder_add_2\n",
    "\n",
    "prediction = tf.keras.layers.Dense(VOCAB_SIZE, activation=\"softmax\")(layer_output)\n",
    "transformer = tf.keras.models.Model(\n",
    "    inputs=decoder_inputs, outputs=prediction, name=\"transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, None, 512)            4819456   ['decoder_inputs[0][0]']      \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, None, 512)            1024      ['positional_embedding[0][0]']\n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_self_attention (De  (None, None, 512)            1680230   ['layer_normalization[0][0]'] \n",
      " coderSelfAttention)                                      4                                       \n",
      "                                                                                                  \n",
      " add (Add)                   (None, None, 512)            0         ['decoder_self_attention[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, None, 512)            1024      ['add[0][0]']                 \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, None, 2048)           1050624   ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, None, 512)            1049088   ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, None, 512)            0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, None, 512)            0         ['dropout[0][0]',             \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, None, 512)            1024      ['add_1[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_self_attention_1 (  (None, None, 512)            1680230   ['layer_normalization_2[0][0]'\n",
      " DecoderSelfAttention)                                    4         ]                             \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, None, 512)            0         ['decoder_self_attention_1[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, None, 512)            1024      ['add_2[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 2048)           1050624   ['layer_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, None, 512)            1049088   ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, None, 512)            0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, None, 512)            0         ['dropout_1[0][0]',           \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, None, 512)            1024      ['add_3[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_self_attention_2 (  (None, None, 512)            1680230   ['layer_normalization_4[0][0]'\n",
      " DecoderSelfAttention)                                    4         ]                             \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, None, 512)            0         ['decoder_self_attention_2[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_5 (Lay  (None, None, 512)            1024      ['add_4[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, None, 2048)           1050624   ['layer_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, None, 512)            1049088   ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, None, 512)            0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, None, 512)            0         ['dropout_2[0][0]',           \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_6 (Lay  (None, None, 512)            1024      ['add_5[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_self_attention_3 (  (None, None, 512)            1680230   ['layer_normalization_6[0][0]'\n",
      " DecoderSelfAttention)                                    4         ]                             \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, None, 512)            0         ['decoder_self_attention_3[0][\n",
      "                                                                    0]',                          \n",
      "                                                                     'layer_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_7 (Lay  (None, None, 512)            1024      ['add_6[0][0]']               \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, None, 2048)           1050624   ['layer_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, None, 512)            1049088   ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, None, 512)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, None, 512)            0         ['dropout_3[0][0]',           \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, None, 8912)           4571856   ['add_7[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 85007568 (324.28 MB)\n",
      "Trainable params: 85007568 (324.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Short Rebuild of the above but with Hugging Face API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might be able to leverage accelerated training by using HF so this could be worth to have.  It also uses a PyTorch backend which I know works on Brandon's system... TensorFlow has been giving me problems in my environment lately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the decoder-only setup\n",
    "from transformers import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(8912, 512)\n",
       "    (wpe): Embedding(500, 512)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=1536, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=512)\n",
       "          (attn_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=2048, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=2048)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=8912, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = 8912 # Define this based on training data vocab size / tokenize size.\n",
    "# TODO: Or does it need to relate to the tokenizer?\n",
    "SEQUENCE_LENGTH = 500 # Define this based on training data sequence length. aka. context window\n",
    "EMBEDDING_DIM = 512 # Configure\n",
    "FEED_FORWARD_DIM = 4 * EMBEDDING_DIM # Standard for decoder-only items.\n",
    "DROPOUT = 0.5 # Configure\n",
    "NUM_HEADS = 16 # Configure attention heads\n",
    "NUM_DECODER_LAYERS = 4 # Configure\n",
    "\n",
    "# Define the model configuration\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=SEQUENCE_LENGTH,\n",
    "    n_ctx=SEQUENCE_LENGTH,\n",
    "    n_embd=EMBEDDING_DIM,\n",
    "    n_layer=NUM_DECODER_LAYERS,\n",
    "    n_head=NUM_HEADS,\n",
    "    resid_pdrop=DROPOUT,\n",
    "    embd_pdrop=DROPOUT,\n",
    "    attn_pdrop=DROPOUT,\n",
    "    # layer_norm_epsilon=1e-5,\n",
    "    # initializer_range=0.02,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "gptmodel = GPT2LMHeadModel(model_config)\n",
    "# Output the model layers for inspection\n",
    "gptmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 17429504\n"
     ]
    }
   ],
   "source": [
    "# How many trainable parameters exist in the gptmodel? (setting p.requires_grad to True for this)\n",
    "trainable_params = sum(p.numel() for p in gptmodel.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight: 4562944 trainable parameters\n",
      "transformer.wpe.weight: 256000 trainable parameters\n",
      "transformer.h.0.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.0.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.0.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.0.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.0.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.0.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.0.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.0.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.0.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.0.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.0.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.0.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.1.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.1.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.1.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.1.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.1.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.1.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.1.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.1.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.1.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.1.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.1.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.1.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.2.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.2.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.2.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.2.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.2.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.2.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.2.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.2.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.2.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.2.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.2.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.2.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.3.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.3.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.3.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.3.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.3.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.3.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.3.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.3.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.3.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.3.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.3.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.3.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.ln_f.weight: 512 trainable parameters\n",
      "transformer.ln_f.bias: 512 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Print each parameter's name and trainable count individually\n",
    "for name, param in gptmodel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference in the reported number of trained parameters between Etienne's custom build and the Hugging Face model.\n",
    "\n",
    "I'm 90% certain it has to do with the positional eoncodings being done slightly different.  48M of his model parameters are from the PosEnc which doesn't show up the same way in the GPT-model.\n",
    "\n",
    "Both models are likely to generate similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize your train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up datasets for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD CODING WORK:\n",
    "\n",
    "# Set up the Trainer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, return_tensors='pt')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_reduced['train'],\n",
    "    eval_dataset=tokenized_dataset_reduced['validation'],0\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
