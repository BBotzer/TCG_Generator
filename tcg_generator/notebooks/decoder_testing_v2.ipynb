{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned Version of a Decoder-only (GPT style) transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "import os\n",
    "from typing import List, Optional, Union\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import transformers\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset, DatasetDict,load_dataset #, load_metric\n",
    "from transformers.tokenization_utils_base import AddedToken\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "import ast # Because the tokenized_text column data is stored as a string instead of a list...\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes use of the Hugging Face transformer.trainer API so it uses a PyTorch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are usisng cuda on a NVIDIA GeForce GTX 1070.\n"
     ]
    }
   ],
   "source": [
    "# Check pytorch GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'You are usisng {device} on a {torch.cuda.get_device_name()}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grab the dataset\n",
    "\n",
    "1) Get the dataset\n",
    "2) Fix the tokenize_text column from a string to a list of strings\n",
    "3) Pull out all of the words into a mega-list\n",
    "\n",
    "**SOMTHING IS WRONG WITH THE DATASET.  Likely an issue with the preprocessing steps...**\n",
    "\n",
    "See:\n",
    "\n",
    "Item 19152 seems like it is too many cards put together... I need to figure out what is going on.\n",
    "\n",
    "'Crash Landing — Search your library for a basic land card , reveal it , put it into your hand , then shuffle . \\\\n Goblin Camp — Create a Treasure token . \\\\n Emerald Grove — Create a 2/2 white Knight creature token . \\\\n Auntie\\'s Teahouse — Scry 3 . \\\\n Defiled Temple — You may sacrifice a permanent . If you do , draw a card . \\\\n Mountain Pass — You may put a land card from your hand onto the battlefield . \\\\n Ebonlake Grotto — Create two 1/1 blue Faerie Dragon creature tokens with flying . \\\\n Grymforge — For each opponent , goad up to one target creature that player controls . \\\\n Githyanki Crèche — Distribute three +1/+1 counters among up to three target creatures you control . \\\\n Last Light Inn — Draw two cards . \\\\n Reithwin Tollhouse — Roll 2d4 and create that many Treasure tokens . \\\\n Moonrise Towers — Instant and sorcery spells you cast this turn cost 3 less to cast . \\\\n Gauntlet of Shar — Each opponent loses 5 life . \\\\n Balthazar\\'s Lab — Return up to two target creature cards from your graveyard to your hand . \\\\n Circus of the Last Days — Create a token that\\'s a copy of one of your commanders , except it\\'s not legendary . \\\\n Undercity Ruins — Create three 4/1 black Skeleton creature tokens with menace . \\\\n Steel Watch Foundry — You get an emblem with \"Creatures you control get +2/+2 and have trample . \" \\\\n Ansur\\'s Sanctum — Reveal the top four cards of your library and put them into your hand . Each opponent loses life equal to those cards\\' total mana value . \\\\n Temple of Bhaal — Creatures your opponents control get -5/-5 until end of turn .'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the processed card data from wherever you have it.\n",
    "\n",
    "# df = pd.read_csv('../data/processed/mtg_carddata_processed.csv')\n",
    "\n",
    "df = pd.read_csv('../data/processed/mtg_carddata_processed_2_23_25.csv')  # Trainer and current process will work with this dataset\n",
    "\n",
    "# df = pd.read_csv('../data/processed/training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mana_cost</th>\n",
       "      <th>type_line</th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>colors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mtgo_id</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>defense</th>\n",
       "      <th>processed_oracle_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tfidf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nissa, Worldsoul Speaker</td>\n",
       "      <td>{3}{G}</td>\n",
       "      <td>Legendary Creature — Elf Druid</td>\n",
       "      <td>Landfall — Whenever a land you control enters,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Landfall']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Landfall — Whenever a land you control enters ...</td>\n",
       "      <td>['Landfall', 'Whenever', 'a', 'land', 'you', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Static Orb</td>\n",
       "      <td>{3}</td>\n",
       "      <td>Artifact</td>\n",
       "      <td>As long as &lt;name&gt; is untapped, players can't u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>15870.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As long as Static Orb is untapped , players ca...</td>\n",
       "      <td>['As', 'long', 'as', '&lt;name&gt;', 'is', 'untapped...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensory Deprivation</td>\n",
       "      <td>{U}</td>\n",
       "      <td>Enchantment — Aura</td>\n",
       "      <td>Enchant creature\\r\\nEnchanted creature gets -3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Enchant']</td>\n",
       "      <td>49283.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enchant creature \\n Enchanted creature gets -3...</td>\n",
       "      <td>['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Road of Return</td>\n",
       "      <td>{G}{G}</td>\n",
       "      <td>Sorcery</td>\n",
       "      <td>Choose one —\\r\\n• Return target permanent card...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Entwine']</td>\n",
       "      <td>77122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Choose one — \\n • Return target permanent card...</td>\n",
       "      <td>['Choose', 'one', '\\\\n', 'Return', 'target', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Storm Crow</td>\n",
       "      <td>{1}{U}</td>\n",
       "      <td>Creature — Bird</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Flying']</td>\n",
       "      <td>22609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>['Flying', 'This', 'creature', \"can't\", 'be', ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30618</th>\n",
       "      <td>Devoted Hero</td>\n",
       "      <td>{W}</td>\n",
       "      <td>Creature — Elf Soldier</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30619</th>\n",
       "      <td>Without Weakness</td>\n",
       "      <td>{1}{B}</td>\n",
       "      <td>Instant</td>\n",
       "      <td>Target creature you control gains indestructib...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['Cycling']</td>\n",
       "      <td>64646.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Target creature you control gains indestructib...</td>\n",
       "      <td>['Target', 'creature', 'you', 'control', 'gain...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30620</th>\n",
       "      <td>Firesong and Sunspeaker</td>\n",
       "      <td>{4}{R}{W}</td>\n",
       "      <td>Legendary Creature — Minotaur Cleric</td>\n",
       "      <td>Red instant and sorcery spells you control hav...</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>['R', 'W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>101914.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Red instant and sorcery spells you control hav...</td>\n",
       "      <td>['Red', 'instant', 'and', 'sorcery', 'spells',...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30621</th>\n",
       "      <td>Samut, the Tested</td>\n",
       "      <td>{2}{R}{G}</td>\n",
       "      <td>Legendary Planeswalker — Samut</td>\n",
       "      <td>+1: Up to one target creature gains double str...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G', 'R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>64772.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>+1 : Up to one target creature gains double st...</td>\n",
       "      <td>['1', ':', 'Up', 'to', 'one', 'target', 'creat...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30622</th>\n",
       "      <td>Sinew Sliver</td>\n",
       "      <td>{1}{W}</td>\n",
       "      <td>Creature — Sliver</td>\n",
       "      <td>All Sliver creatures get +1/+1.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>114015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All Sliver creatures get +1/+1 .</td>\n",
       "      <td>['All', 'Sliver', 'creatures', 'get', '1', '1'...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30623 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  mana_cost  \\\n",
       "0      Nissa, Worldsoul Speaker     {3}{G}   \n",
       "1                    Static Orb        {3}   \n",
       "2           Sensory Deprivation        {U}   \n",
       "3                Road of Return     {G}{G}   \n",
       "4                    Storm Crow     {1}{U}   \n",
       "...                         ...        ...   \n",
       "30618              Devoted Hero        {W}   \n",
       "30619          Without Weakness     {1}{B}   \n",
       "30620   Firesong and Sunspeaker  {4}{R}{W}   \n",
       "30621         Samut, the Tested  {2}{R}{G}   \n",
       "30622              Sinew Sliver     {1}{W}   \n",
       "\n",
       "                                  type_line  \\\n",
       "0            Legendary Creature — Elf Druid   \n",
       "1                                  Artifact   \n",
       "2                        Enchantment — Aura   \n",
       "3                                   Sorcery   \n",
       "4                           Creature — Bird   \n",
       "...                                     ...   \n",
       "30618                Creature — Elf Soldier   \n",
       "30619                               Instant   \n",
       "30620  Legendary Creature — Minotaur Cleric   \n",
       "30621        Legendary Planeswalker — Samut   \n",
       "30622                     Creature — Sliver   \n",
       "\n",
       "                                             oracle_text power toughness  \\\n",
       "0      Landfall — Whenever a land you control enters,...     3         3   \n",
       "1      As long as <name> is untapped, players can't u...   NaN       NaN   \n",
       "2      Enchant creature\\r\\nEnchanted creature gets -3...   NaN       NaN   \n",
       "3      Choose one —\\r\\n• Return target permanent card...   NaN       NaN   \n",
       "4      Flying (This creature can't be blocked except ...     1         2   \n",
       "...                                                  ...   ...       ...   \n",
       "30618                                                NaN     1         2   \n",
       "30619  Target creature you control gains indestructib...   NaN       NaN   \n",
       "30620  Red instant and sorcery spells you control hav...     4         6   \n",
       "30621  +1: Up to one target creature gains double str...   NaN       NaN   \n",
       "30622                    All Sliver creatures get +1/+1.     1         1   \n",
       "\n",
       "           colors      keywords   mtgo_id loyalty  defense  \\\n",
       "0           ['G']  ['Landfall']       NaN     NaN      NaN   \n",
       "1              []            []   15870.0     NaN      NaN   \n",
       "2           ['U']   ['Enchant']   49283.0     NaN      NaN   \n",
       "3           ['G']   ['Entwine']   77122.0     NaN      NaN   \n",
       "4           ['U']    ['Flying']   22609.0     NaN      NaN   \n",
       "...           ...           ...       ...     ...      ...   \n",
       "30618       ['W']            []       NaN     NaN      NaN   \n",
       "30619       ['B']   ['Cycling']   64646.0     NaN      NaN   \n",
       "30620  ['R', 'W']            []  101914.0     NaN      NaN   \n",
       "30621  ['G', 'R']            []   64772.0       4      NaN   \n",
       "30622       ['W']            []  114015.0     NaN      NaN   \n",
       "\n",
       "                                   processed_oracle_text  \\\n",
       "0      Landfall — Whenever a land you control enters ...   \n",
       "1      As long as Static Orb is untapped , players ca...   \n",
       "2      Enchant creature \\n Enchanted creature gets -3...   \n",
       "3      Choose one — \\n • Return target permanent card...   \n",
       "4      Flying (This creature can't be blocked except ...   \n",
       "...                                                  ...   \n",
       "30618                                                NaN   \n",
       "30619  Target creature you control gains indestructib...   \n",
       "30620  Red instant and sorcery spells you control hav...   \n",
       "30621  +1 : Up to one target creature gains double st...   \n",
       "30622                   All Sliver creatures get +1/+1 .   \n",
       "\n",
       "                                          tokenized_text  \\\n",
       "0      ['Landfall', 'Whenever', 'a', 'land', 'you', '...   \n",
       "1      ['As', 'long', 'as', '<name>', 'is', 'untapped...   \n",
       "2      ['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...   \n",
       "3      ['Choose', 'one', '\\\\n', 'Return', 'target', '...   \n",
       "4      ['Flying', 'This', 'creature', \"can't\", 'be', ...   \n",
       "...                                                  ...   \n",
       "30618                                                 []   \n",
       "30619  ['Target', 'creature', 'you', 'control', 'gain...   \n",
       "30620  ['Red', 'instant', 'and', 'sorcery', 'spells',...   \n",
       "30621  ['1', ':', 'Up', 'to', 'one', 'target', 'creat...   \n",
       "30622  ['All', 'Sliver', 'creatures', 'get', '1', '1'...   \n",
       "\n",
       "                  tfidf_vector  \n",
       "0      [0. 0. 0. ... 0. 0. 0.]  \n",
       "1      [0. 0. 0. ... 0. 0. 0.]  \n",
       "2      [0. 0. 0. ... 0. 0. 0.]  \n",
       "3      [0. 0. 0. ... 0. 0. 0.]  \n",
       "4      [0. 0. 0. ... 0. 0. 0.]  \n",
       "...                        ...  \n",
       "30618  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30619  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30620  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30621  [0. 0. 0. ... 0. 0. 0.]  \n",
       "30622  [0. 0. 0. ... 0. 0. 0.]  \n",
       "\n",
       "[30623 rows x 14 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mana_cost</th>\n",
       "      <th>type_line</th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>colors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>mtgo_id</th>\n",
       "      <th>loyalty</th>\n",
       "      <th>defense</th>\n",
       "      <th>processed_oracle_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tfidf_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nissa, Worldsoul Speaker</td>\n",
       "      <td>{3}{G}</td>\n",
       "      <td>Legendary Creature — Elf Druid</td>\n",
       "      <td>Landfall — Whenever a land you control enters,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Landfall']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Landfall — Whenever a land you control enters ...</td>\n",
       "      <td>['Landfall', 'Whenever', 'a', 'land', 'you', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Static Orb</td>\n",
       "      <td>{3}</td>\n",
       "      <td>Artifact</td>\n",
       "      <td>As long as &lt;name&gt; is untapped, players can't u...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>15870.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As long as Static Orb is untapped , players ca...</td>\n",
       "      <td>['As', 'long', 'as', '&lt;name&gt;', 'is', 'untapped...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sensory Deprivation</td>\n",
       "      <td>{U}</td>\n",
       "      <td>Enchantment — Aura</td>\n",
       "      <td>Enchant creature\\r\\nEnchanted creature gets -3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Enchant']</td>\n",
       "      <td>49283.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enchant creature \\n Enchanted creature gets -3...</td>\n",
       "      <td>['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Road of Return</td>\n",
       "      <td>{G}{G}</td>\n",
       "      <td>Sorcery</td>\n",
       "      <td>Choose one —\\r\\n• Return target permanent card...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['Entwine']</td>\n",
       "      <td>77122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Choose one — \\n • Return target permanent card...</td>\n",
       "      <td>['Choose', 'one', '\\\\n', 'Return', 'target', '...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Storm Crow</td>\n",
       "      <td>{1}{U}</td>\n",
       "      <td>Creature — Bird</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['Flying']</td>\n",
       "      <td>22609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flying (This creature can't be blocked except ...</td>\n",
       "      <td>['Flying', 'This', 'creature', \"can't\", 'be', ...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Snarlfang Vermin</td>\n",
       "      <td>{B}</td>\n",
       "      <td>Creature — Rat</td>\n",
       "      <td>Whenever &lt;name&gt; deals combat damage to a creat...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['B']</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Whenever Snarlfang Vermin deals combat damage ...</td>\n",
       "      <td>['Whenever', '&lt;name&gt;', 'deals', 'combat', 'dam...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Walking Sponge</td>\n",
       "      <td>{1}{U}</td>\n",
       "      <td>Creature — Sponge</td>\n",
       "      <td>{T}: Target creature loses your choice of flyi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['U']</td>\n",
       "      <td>[]</td>\n",
       "      <td>12637.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{T} : Target creature loses your choice of fly...</td>\n",
       "      <td>['{T}', ':', 'Target', 'creature', 'loses', 'y...</td>\n",
       "      <td>[0. 0. 0. ... 0. 0. 0.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name mana_cost                       type_line  \\\n",
       "0  Nissa, Worldsoul Speaker    {3}{G}  Legendary Creature — Elf Druid   \n",
       "1                Static Orb       {3}                        Artifact   \n",
       "2       Sensory Deprivation       {U}              Enchantment — Aura   \n",
       "3            Road of Return    {G}{G}                         Sorcery   \n",
       "4                Storm Crow    {1}{U}                 Creature — Bird   \n",
       "5          Snarlfang Vermin       {B}                  Creature — Rat   \n",
       "6            Walking Sponge    {1}{U}               Creature — Sponge   \n",
       "\n",
       "                                         oracle_text power toughness colors  \\\n",
       "0  Landfall — Whenever a land you control enters,...     3         3  ['G']   \n",
       "1  As long as <name> is untapped, players can't u...   NaN       NaN     []   \n",
       "2  Enchant creature\\r\\nEnchanted creature gets -3...   NaN       NaN  ['U']   \n",
       "3  Choose one —\\r\\n• Return target permanent card...   NaN       NaN  ['G']   \n",
       "4  Flying (This creature can't be blocked except ...     1         2  ['U']   \n",
       "5  Whenever <name> deals combat damage to a creat...     2         1  ['B']   \n",
       "6  {T}: Target creature loses your choice of flyi...     1         1  ['U']   \n",
       "\n",
       "       keywords  mtgo_id loyalty  defense  \\\n",
       "0  ['Landfall']      NaN     NaN      NaN   \n",
       "1            []  15870.0     NaN      NaN   \n",
       "2   ['Enchant']  49283.0     NaN      NaN   \n",
       "3   ['Entwine']  77122.0     NaN      NaN   \n",
       "4    ['Flying']  22609.0     NaN      NaN   \n",
       "5            []      NaN     NaN      NaN   \n",
       "6            []  12637.0     NaN      NaN   \n",
       "\n",
       "                               processed_oracle_text  \\\n",
       "0  Landfall — Whenever a land you control enters ...   \n",
       "1  As long as Static Orb is untapped , players ca...   \n",
       "2  Enchant creature \\n Enchanted creature gets -3...   \n",
       "3  Choose one — \\n • Return target permanent card...   \n",
       "4  Flying (This creature can't be blocked except ...   \n",
       "5  Whenever Snarlfang Vermin deals combat damage ...   \n",
       "6  {T} : Target creature loses your choice of fly...   \n",
       "\n",
       "                                      tokenized_text             tfidf_vector  \n",
       "0  ['Landfall', 'Whenever', 'a', 'land', 'you', '...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "1  ['As', 'long', 'as', '<name>', 'is', 'untapped...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "2  ['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "3  ['Choose', 'one', '\\\\n', 'Return', 'target', '...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "4  ['Flying', 'This', 'creature', \"can't\", 'be', ...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "5  ['Whenever', '<name>', 'deals', 'combat', 'dam...  [0. 0. 0. ... 0. 0. 0.]  \n",
       "6  ['{T}', ':', 'Target', 'creature', 'loses', 'y...  [0. 0. 0. ... 0. 0. 0.]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized_text here isn't really tokenized... it is just the text split into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['Landfall', 'Whenever', 'a', 'land', 'you', '...\n",
      "1    ['As', 'long', 'as', '<name>', 'is', 'untapped...\n",
      "2    ['Enchant', 'creature', '\\\\n', 'Enchanted', 'c...\n",
      "3    ['Choose', 'one', '\\\\n', 'Return', 'target', '...\n",
      "4    ['Flying', 'This', 'creature', \"can't\", 'be', ...\n",
      "5    ['Whenever', '<name>', 'deals', 'combat', 'dam...\n",
      "6    ['{T}', ':', 'Target', 'creature', 'loses', 'y...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df['tokenized_text'][:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Landfall', 'Whenever', 'a', 'land', 'you', 'control', 'enters', ',', 'you', 'get', '{E}', '{E}', 'two', 'energy', 'counters', '.', '\\\\\\\\n', 'You', 'may', 'pay', 'eight', '{E}', 'rather', 'than', 'pay', 'the', 'mana', 'cost', 'for', 'permanent', 'spells', 'you', 'cast', '.']\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Landfall, Whenever, a, land, you, control, en...\n",
       "1    [As, long, as, <name>, is, untapped, ,, player...\n",
       "2    [Enchant, creature, \\n, Enchanted, creature, g...\n",
       "3    [Choose, one, \\n, Return, target, permanent, c...\n",
       "4    [Flying, This, creature, can't, be, blocked, e...\n",
       "5    [Whenever, <name>, deals, combat, damage, to, ...\n",
       "6    [{T}, :, Target, creature, loses, your, choice...\n",
       "7            [Exile, all, multicolored, permanents, .]\n",
       "8    [When, <name>, enters, ,, create, a, Food, tok...\n",
       "9    [<name>, deals, damage, to, any, target, equal...\n",
       "Name: tokenized_text, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply ast.literal_eval to each row in the tokenized_text column\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(ast.literal_eval)\n",
    "\n",
    "# Display first few rows to verify\n",
    "df['tokenized_text'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landfall'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_text'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 968000\n",
      "First 20 tokens: ['Landfall', 'Whenever', 'a', 'land', 'you', 'control', 'enters', ',', 'you', 'get', '{E}', '{E}', 'two', 'energy', 'counters', '.', '\\\\n', 'You', 'may', 'pay']\n"
     ]
    }
   ],
   "source": [
    "# Combine all tokens into one large list\n",
    "all_tokens = []\n",
    "for tokens in df['tokenized_text']:\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Alternative one-liner using list comprehension\n",
    "# all_tokens = [token for tokens in df['tokenized_text'] for token in tokens]\n",
    "\n",
    "# Display the first 20 tokens to verify\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(\"First 20 tokens:\", all_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Landfall — Whenever a land you control enters ...\n",
      "1    As long as Static Orb is untapped , players ca...\n",
      "2    Enchant creature \\n Enchanted creature gets -3...\n",
      "Name: processed_oracle_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus = df['processed_oracle_text']\n",
    "print(corpus[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert corpus to list of strings if it's not already\n",
    "corpus_list = corpus.tolist() if hasattr(corpus, 'tolist') else list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas Series to list of strings\n",
    "corpus_list = corpus.values.tolist()\n",
    "\n",
    "# Ensure all elements are strings\n",
    "corpus_list = [str(text) for text in corpus_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Landfall — Whenever a land you control enters , you get {E} {E} (two energy counters) . \\\\n You may pay eight {E} rather than pay the mana cost for permanent spells you cast .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Corrected the above code to ensure all elements are strings\n",
    "corpus_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Check the first few elements to verify\n",
    "print(type(corpus_list))  # Should show: <class 'list'>\n",
    "print(type(corpus_list[0]))  # Should show: <class 'str'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/val/test datasets\n",
    "\n",
    "Do we need to keep other information with the corpus_list as we split it up?  Names, color, etc.?\n",
    "\n",
    "If we're just generating text, I think that answer is no.  If we want more, then the answer is likely yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train/validation/test (80/10/10) sets\n",
    "train_list, temp = train_test_split(corpus_list, test_size=0.2, random_state=42) # Set 80% for training\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42) # Set 10% for validation and 10% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 24498\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 3062\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 3063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn them into dicts so I can use them with the Hugging Face Dataset class\n",
    "train = [{\"sentence\": text} for text in train_list]  # Wrap each sentence in a dict\n",
    "val = [{\"sentence\": text} for text in val]\n",
    "test = [{\"sentence\": text} for text in test]\n",
    "\n",
    "# Create the Dataset Dictionary for future mapping\n",
    "data = DatasetDict({\n",
    "    'train': Dataset.from_list(train),\n",
    "    'validation': Dataset.from_list(val),\n",
    "    'test': Dataset.from_list(test)\n",
    "    })\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': ['Choose one — \\\\n • Destroy target Cleric . \\\\n • Return target Cleric card from your graveyard to your hand . \\\\n • Target player loses 2 life .',\n",
       "  '( {T} : Add {W} or {B} . )',\n",
       "  'Explosive Welcome deals 5 damage to any target and 3 damage to any other target . Add {R} {R} {R} .',\n",
       "  \"Vigilance (Attacking doesn't cause this creature to tap . )\",\n",
       "  \"Trample \\\\n Alexios , Deimos of Kosmos attacks each combat if able , can't be sacrificed , and can't attack its owner . \\\\n At the beginning of each player's upkeep , that player gains control of Alexios , untaps it , and puts a +1/+1 counter on it . It gains haste until end of turn .\"]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the items\n",
    "data['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flash \\\\n Other white creatures you control get +1/+1 . \\\\n Other blue creatures you control get +1/+1 .'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['validation']['sentence'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Tokenizer on Training Data\n",
    "\n",
    "Keep the validation and test sets out of the tokenizer to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for tokenizer files if it doesn't exist\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Functions\n",
    "\n",
    "1) `create_gpt2_tokenizer`\n",
    "\n",
    "2) `train_tokenizer_from_texts`\n",
    "\n",
    "3) `convert_to_transformers_tokenizer` -> This is important to allow `trainer.train` to work\n",
    "\n",
    "4) `use_transformers_tokenizer`\n",
    "\n",
    "5) `load_transformers_tokenizer` -> Used if you have one to load, else just carry from `tokenizer=convert_to_transformers_tokenizer`\n",
    "\n",
    "6) `demonstrate_workflow2` -> make sure this whole thing even works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gpt2_tokenizer(vocab_size=8192, min_frequency=2):\n",
    "    \"\"\"\n",
    "    Create a GPT2-style BPE tokenizer from scratch using Hugging Face tokenizers library.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: The size of the vocabulary to learn\n",
    "        min_frequency: Minimum frequency for a token to be considered in the BPE algorithm\n",
    "\n",
    "    Returns:\n",
    "        A tokenizer object ready for training\n",
    "    \"\"\"\n",
    "    # Initialize a ByteLevelBPE model-based tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Add byte-level pre-tokenizer\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "    # Add decoder to properly decode byte-level tokens\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # Set up normalizers - GPT-2 doesn't do much normalization\n",
    "    tokenizer.normalizer = Sequence([NFKC()])\n",
    "\n",
    "    # Return the tokenizer (to be trained later)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def train_tokenizer_from_texts(\n",
    "    tokenizer: Tokenizer,\n",
    "    texts: List[str],\n",
    "    vocab_size: int = 8192,\n",
    "    min_frequency: int = 2,\n",
    "    batch_size: int = 512,\n",
    "    output_dir: str = \"tokenizer\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the tokenizer on a list of texts using batching\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer object to train\n",
    "        texts: List of strings to use for training\n",
    "        vocab_size: Maximum vocabulary size\n",
    "        min_frequency: Minimum frequency for a token\n",
    "        batch_size: Number of texts to process in each batch\n",
    "        output_dir: Directory to save the trained tokenizer\n",
    "\n",
    "    Returns:\n",
    "        The trained tokenizer\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        raise ValueError(\"No texts provided for training\")\n",
    "\n",
    "    # Configure the BPE trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=[\"<|endoftext|>\", \"<|pad|>\"],\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    # Create temporary files for batched training\n",
    "    temp_dir = os.path.join(output_dir, \"temp\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Split texts into batches\n",
    "    batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]\n",
    "    batch_files = []\n",
    "\n",
    "    print(f\"Preparing {len(batches)} batches for training...\")\n",
    "\n",
    "    # Write batches to temporary files\n",
    "    for i, batch in enumerate(tqdm(batches)):\n",
    "        batch_file = os.path.join(temp_dir, f\"batch_{i}.txt\")\n",
    "        with open(batch_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n<|endoftext|>\\n\".join(batch))\n",
    "            f.write(\"\\n<|endoftext|>\\n\")  # Add final separator\n",
    "        batch_files.append(batch_file)\n",
    "\n",
    "    # Train the tokenizer\n",
    "    print(\"Training tokenizer...\")\n",
    "    tokenizer.train(batch_files, trainer)\n",
    "\n",
    "    # Add post-processor to handle special tokens\n",
    "    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "\n",
    "    # Enable padding with the PAD token\n",
    "    pad_id = tokenizer.token_to_id(\"<|pad|>\")\n",
    "    if pad_id is not None:\n",
    "        tokenizer.enable_padding(pad_id=pad_id, pad_token=\"<|pad|>\")\n",
    "    else:\n",
    "        print(\"Warning: <|pad|> token not found in vocabulary. Padding won't work correctly.\")\n",
    "\n",
    "    # Save the trained tokenizer\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    tokenizer.save(f\"{output_dir}/gpt2_tokenizer.json\")\n",
    "\n",
    "    # Clean up temporary files\n",
    "    for file in batch_files:\n",
    "        os.remove(file)\n",
    "    os.rmdir(temp_dir)\n",
    "\n",
    "    print(f\"Tokenizer trained and saved to {output_dir}/gpt2_tokenizer.json\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def convert_to_transformers_tokenizer(\n",
    "    tokenizer_path: str,\n",
    "    output_dir: str = None,\n",
    "    model_max_length: int = 1024\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert a trained tokenizers.Tokenizer to a transformers compatible tokenizer\n",
    "\n",
    "    Args:\n",
    "        tokenizer_path: Path to the saved tokenizer JSON file\n",
    "        output_dir: Directory to save the transformers tokenizer\n",
    "        model_max_length: Maximum sequence length for the model\n",
    "\n",
    "    Returns:\n",
    "        A transformers-compatible tokenizer\n",
    "    \"\"\"\n",
    "    # Load the tokenizer\n",
    "    if isinstance(tokenizer_path, Tokenizer):\n",
    "        # If a tokenizer object is passed directly\n",
    "        raw_tokenizer = tokenizer_path\n",
    "    else:\n",
    "        # Load from file\n",
    "        raw_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "    # Define special tokens\n",
    "    eos_token = \"<|endoftext|>\"  # GPT-2 uses EOS as both BOS and EOS\n",
    "    pad_token = \"<|pad|>\"\n",
    "\n",
    "    # Create the transformers wrapper\n",
    "    transformers_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=raw_tokenizer,\n",
    "        bos_token=eos_token,  # GPT-2 uses EOS as BOS too\n",
    "        eos_token=eos_token,\n",
    "        pad_token=pad_token,\n",
    "        unk_token=eos_token,  # GPT-2 doesn't really have UNK, defaults to EOS\n",
    "        model_max_length=model_max_length\n",
    "    )\n",
    "\n",
    "    # Set additional GPT-2 specific attributes\n",
    "    transformers_tokenizer.add_special_tokens({\n",
    "        \"eos_token\": eos_token,\n",
    "        \"bos_token\": eos_token,\n",
    "        \"pad_token\": pad_token,\n",
    "    })\n",
    "\n",
    "    # Save the transformers tokenizer if an output directory is provided\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        transformers_tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Transformers tokenizer saved to {output_dir}\")\n",
    "\n",
    "    return transformers_tokenizer\n",
    "\n",
    "\n",
    "def use_transformers_tokenizer(tokenizer, text, padding=True, truncation=True, max_length=None, return_tensors=None):\n",
    "    \"\"\"\n",
    "    Demonstrate how to use the transformers tokenizer\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The transformers tokenizer\n",
    "        text: Text to tokenize or list of texts\n",
    "        padding: Whether to pad the sequences (True, False, 'max_length', 'longest')\n",
    "        truncation: Whether to truncate sequences (True, False)\n",
    "        max_length: Maximum length for padding/truncation (defaults to model_max_length if None)\n",
    "        return_tensors: Return format for tensors ('pt' for PyTorch, 'tf' for TensorFlow, None for lists)\n",
    "\n",
    "    Returns:\n",
    "        Encoding object(s) with tokens, ids, etc.\n",
    "    \"\"\"\n",
    "    # Configure parameters\n",
    "    if max_length is None:\n",
    "        max_length = tokenizer.model_max_length\n",
    "\n",
    "    # Encode the text(s)\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        max_length=max_length,\n",
    "        return_tensors=return_tensors\n",
    "    )\n",
    "\n",
    "    # Display results for single text if it's a string\n",
    "    if isinstance(text, str):\n",
    "        print(f\"Input text: {text}\")\n",
    "        print(f\"Token IDs: {encoded['input_ids'].tolist()[0] if return_tensors else encoded['input_ids']}\")\n",
    "        tokens = tokenizer.convert_ids_to_tokens(\n",
    "            encoded['input_ids'].tolist()[0] if return_tensors else encoded['input_ids']\n",
    "        )\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "\n",
    "        if padding:\n",
    "            print(f\"Attention mask: {encoded['attention_mask'].tolist()[0] if return_tensors else encoded['attention_mask']}\")\n",
    "\n",
    "        # Decoding demonstration\n",
    "        decoded = tokenizer.decode(\n",
    "            encoded['input_ids'].tolist()[0] if return_tensors else encoded['input_ids'],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        print(f\"Decoded text: {decoded}\")\n",
    "\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def load_transformers_tokenizer(path):\n",
    "    \"\"\"\n",
    "    Load a previously saved transformers tokenizer\n",
    "\n",
    "    Args:\n",
    "        path: Path to the saved transformers tokenizer directory\n",
    "\n",
    "    Returns:\n",
    "        The loaded transformers tokenizer\n",
    "    \"\"\"\n",
    "    return GPT2TokenizerFast.from_pretrained(path)\n",
    "\n",
    "\n",
    "def demonstrate_workflow2():\n",
    "    \"\"\"\n",
    "    Full demonstration of creating, training, and using a GPT-2 style tokenizer\n",
    "    that's compatible with the transformers library\n",
    "    \"\"\"\n",
    "    # Create a new tokenizer\n",
    "    print(\"Creating tokenizer...\")\n",
    "    tokenizer = create_gpt2_tokenizer()\n",
    "\n",
    "    # Generate some sample texts for demonstration\n",
    "    print(\"Generating sample texts...\")\n",
    "    sample_texts = [\n",
    "        \"This is an example of text that could be used to train a tokenizer.\",\n",
    "        \"It should include diverse vocabulary, punctuation (like commas, periods, question marks?).\",\n",
    "        \"Multiple paragraphs are good to include.\",\n",
    "        \"Numbers like 42, 3.14159, and 2023 should be represented.\",\n",
    "        \"Code snippets might be important if your model will process code:\\ndef hello_world():\\n    print('Hello, world!')\",\n",
    "    ]\n",
    "\n",
    "    # Add some more generated texts for better training\n",
    "    for i in range(50):\n",
    "        words = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\",\n",
    "                \"Hello\", \"world\", \"Python\", \"programming\", \"is\", \"fun\", \"GPT\",\n",
    "                \"natural\", \"language\", \"processing\", \"models\", \"work\", \"well\"]\n",
    "        length = random.randint(5, 20)\n",
    "        text = \" \".join(random.choices(words, k=length)) + \".\"\n",
    "        sample_texts.append(text)\n",
    "\n",
    "    # Train the tokenizer on the sample texts\n",
    "    print(f\"Training tokenizer on {len(sample_texts)} texts...\")\n",
    "    raw_tokenizer = train_tokenizer_from_texts(\n",
    "        tokenizer,\n",
    "        sample_texts,\n",
    "        vocab_size=1000,  # Smaller vocab for demo\n",
    "        min_frequency=1,\n",
    "        batch_size=20,\n",
    "        output_dir=\"tokenizer_demo\"\n",
    "    )\n",
    "\n",
    "    # Convert to transformers tokenizer\n",
    "    print(\"\\nConverting to transformers tokenizer...\")\n",
    "    transformers_tokenizer = convert_to_transformers_tokenizer(\n",
    "        \"tokenizer_demo/gpt2_tokenizer.json\",\n",
    "        output_dir=\"transformers_tokenizer_demo\",\n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    # Test the transformers tokenizer\n",
    "    print(\"\\nTesting transformers tokenizer...\")\n",
    "    test_texts = [\n",
    "        \"This is a short text.\",\n",
    "        \"This is a slightly longer text with more content.\",\n",
    "        \"Let's see how padding works on texts of different lengths.\"\n",
    "    ]\n",
    "\n",
    "    # Encode with transformers tokenizer (return PyTorch tensors)\n",
    "    print(\"\\nEncoding with PyTorch tensors:\")\n",
    "    encodings = use_transformers_tokenizer(\n",
    "        transformers_tokenizer,\n",
    "        test_texts,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Show that it can be saved and loaded\n",
    "    print(\"\\nLoading saved transformers tokenizer:\")\n",
    "    loaded_tokenizer = load_transformers_tokenizer(\"transformers_tokenizer_demo\")\n",
    "    test_encode = loaded_tokenizer(\n",
    "        \"Testing that loading works correctly.\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    print(f\"Encoded IDs: {test_encode['input_ids'].tolist()[0]}\")\n",
    "    print(f\"Decoded: {loaded_tokenizer.decode(test_encode['input_ids'][0])}\")\n",
    "\n",
    "    return transformers_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tokenizer...\n",
      "Generating sample texts...\n",
      "Training tokenizer on 55 texts...\n",
      "Preparing 3 batches for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 1500.65it/s]\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizer'. \n",
      "The class this function is called from is 'GPT2TokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n",
      "Tokenizer trained and saved to tokenizer_demo/gpt2_tokenizer.json\n",
      "\n",
      "Converting to transformers tokenizer...\n",
      "Transformers tokenizer saved to transformers_tokenizer_demo\n",
      "\n",
      "Testing transformers tokenizer...\n",
      "\n",
      "Encoding with PyTorch tensors:\n",
      "\n",
      "Loading saved transformers tokenizer:\n",
      "Encoded IDs: [26, 124, 47, 79, 287, 57, 39, 42, 28, 31, 79, 138, 46, 166, 67, 180, 217, 39, 52, 7]\n",
      "Decoded: Testing that loading works correctly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=339, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if this even works...\n",
    "demonstrate_workflow2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 48 batches for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 396.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer trained and saved to ../models/gpt2_tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# Train a tokenizer if you don't have one yet\n",
    "trained_tokenizer = create_gpt2_tokenizer(vocab_size=8192, min_frequency=2)\n",
    "\n",
    "# This saves the tokenizer out to memory as well\n",
    "trained_tokenizer = train_tokenizer_from_texts(tokenizer=trained_tokenizer, texts=data['train']['sentence'], batch_size=512, output_dir=models_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers tokenizer saved to ../models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=8192, model_max_length=256, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|pad|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<|pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This pulls the trained_tokenizer and makes it compatable with the transformers library for training\n",
    "# Needs to be a PreTrainedTokenizerFast object\n",
    "tokenizer_fast = convert_to_transformers_tokenizer(tokenizer_path=f\"{models_dir}/gpt2_tokenizer.json\",\n",
    "                                                   output_dir=models_dir, model_max_length=256)\n",
    "\n",
    "# Check the tokenizer\n",
    "tokenizer_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: {2} {U} : Daring Saboteur can't be blocked this turn . \\n Whenever Daring Saboteur deals combat damage to a player , you may draw a card . If you do , discard a card .\n",
      "Token IDs: [87, 17, 89, 168, 51, 89, 208, 4635, 3816, 1203, 188, 322, 293, 238, 485, 214, 219, 134, 166, 74, 341, 4635, 3816, 1203, 188, 328, 434, 279, 173, 133, 271, 143, 147, 257, 407, 133, 180, 134, 357, 147, 435, 143, 507, 133, 180, 134]\n",
      "Tokens: ['{', '2', '}', 'Ġ{', 'U', '}', 'Ġ:', 'ĠDaring', 'ĠSab', 'ote', 'ur', 'Ġcan', \"'t\", 'Ġbe', 'Ġblocked', 'Ġthis', 'Ġturn', 'Ġ.', 'Ġ\\\\', 'n', 'ĠWhenever', 'ĠDaring', 'ĠSab', 'ote', 'ur', 'Ġdeals', 'Ġcombat', 'Ġdamage', 'Ġto', 'Ġa', 'Ġplayer', 'Ġ,', 'Ġyou', 'Ġmay', 'Ġdraw', 'Ġa', 'Ġcard', 'Ġ.', 'ĠIf', 'Ġyou', 'Ġdo', 'Ġ,', 'Ġdiscard', 'Ġa', 'Ġcard', 'Ġ.']\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded text: {2} {U} : Daring Saboteur can't be blocked this turn . \\n Whenever Daring Saboteur deals combat damage to a player , you may draw a card . If you do , discard a card .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  87,   17,   89,  168,   51,   89,  208, 4635, 3816, 1203,  188,  322,\n",
       "          293,  238,  485,  214,  219,  134,  166,   74,  341, 4635, 3816, 1203,\n",
       "          188,  328,  434,  279,  173,  133,  271,  143,  147,  257,  407,  133,\n",
       "          180,  134,  357,  147,  435,  143,  507,  133,  180,  134]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure the fast tokenizer is working\n",
    "encodings = use_transformers_tokenizer(tokenizer_fast, data['validation']['sentence'][1], padding=True, max_length=1024, return_tensors='pt')\n",
    "encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize all of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset_dict(dataset_dict, tokenizer, max_length=None, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Tokenize all sentence entries in a Hugging Face DatasetDict.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict: A Hugging Face DatasetDict containing data with 'sentence' column\n",
    "        tokenizer: A transformers tokenizer\n",
    "        max_length: Maximum sequence length for tokenization (None = no limit)\n",
    "        batch_size: Batch size for processing\n",
    "\n",
    "    Returns:\n",
    "        A new DatasetDict with tokenized data\n",
    "    \"\"\"\n",
    "    from datasets import DatasetDict\n",
    "\n",
    "    # Function to tokenize a batch of examples\n",
    "    def tokenize_function(examples):\n",
    "        # For transformers tokenizers use the __call__ method\n",
    "        return tokenizer(\n",
    "            examples[\"sentence\"],\n",
    "            padding=\"max_length\" if max_length else False,\n",
    "            truncation=True if max_length else False,\n",
    "            max_length=max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "        )\n",
    "\n",
    "    # Create a new DatasetDict to store the tokenized datasets\n",
    "    tokenized_datasets = DatasetDict()\n",
    "\n",
    "    # Process each split in the dataset\n",
    "    for split_name, dataset in dataset_dict.items():\n",
    "        # Map the tokenize function over the dataset in batches\n",
    "        tokenized_datasets[split_name] = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            # remove_columns=[\"sentence\"] if \"sentence\" in dataset.column_names else None, # Keep the sentences\n",
    "            desc=f\"Tokenizing {split_name} split\",\n",
    "        )\n",
    "\n",
    "        # Print the first example to verify\n",
    "        if len(tokenized_datasets[split_name]) > 0:\n",
    "            print(f\"\\nExample from {split_name} split:\")\n",
    "            example = tokenized_datasets[split_name][0]\n",
    "            print(f\"Input IDs (first 10): {example['input_ids'][:10]}...\")\n",
    "            print(f\"Attention Mask (first 10): {example['attention_mask'][:10]}...\")\n",
    "\n",
    "            # Decode a sample for verification\n",
    "            decoded = tokenizer.decode(example['input_ids'])\n",
    "            print(f\"Decoded sample preview: {decoded[:50]}...\")\n",
    "\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Datasets\n",
    "\n",
    "Do we need to do this?  I think so... I can train when I do this so... yes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6639c6e876cf4c77bcadc463c3bdf0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train split:   0%|          | 0/24498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from train split:\n",
      "Input IDs (first 10): [691, 371, 397, 166, 74, 511, 701, 213, 1389, 134]...\n",
      "Attention Mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Decoded sample preview: Choose one — \\n • Destroy target Cleric . \\n • Ret...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550a6f8046e741319754a1ce475c3f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation split:   0%|          | 0/3062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from validation split:\n",
      "Input IDs (first 10): [792, 166, 74, 840, 607, 307, 147, 201, 283, 244]...\n",
      "Attention Mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Decoded sample preview: Flash \\n Other white creatures you control get +1/...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afea8d48c1b4e3cb9a4344022989220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test split:   0%|          | 0/3063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from test split:\n",
      "Input IDs (first 10): [575, 80, 236, 213, 1038, 913, 173, 133, 161, 218]...\n",
      "Attention Mask (first 10): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]...\n",
      "Decoded sample preview: Attach target Aura attached to a creature or land ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 24498\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 3062\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
       "        num_rows: 3063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the DatasetDict\n",
    "# TODO: We can probably decrease the max_length here to 128 or 64 to save memory; We'll have to test it out\n",
    "# Or maybe we don't use a max length...?? TODO: Figure out how to handle this\n",
    "tokenize_dataset_dict = tokenize_dataset_dict(data, tokenizer_fast, max_length=256, batch_size=1000)\n",
    "tokenize_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flash \\\\n Other white creatures you control get +1/+1 . \\\\n Other blue creatures you control get +1/+1 .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure 'sentnece' is still there\n",
    "tokenize_dataset_dict['validation']['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[792,\n",
       " 166,\n",
       " 74,\n",
       " 840,\n",
       " 607,\n",
       " 307,\n",
       " 147,\n",
       " 201,\n",
       " 283,\n",
       " 244,\n",
       " 16,\n",
       " 246,\n",
       " 16,\n",
       " 134,\n",
       " 166,\n",
       " 74,\n",
       " 840,\n",
       " 754,\n",
       " 307,\n",
       " 147,\n",
       " 201,\n",
       " 283,\n",
       " 244,\n",
       " 16,\n",
       " 246,\n",
       " 16,\n",
       " 134,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_dataset_dict['validation']['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short function to easily convert from ids_to_tokens\n",
    "# It effectivly just wraps the decode function\n",
    "def ids_to_tokens(tokenizer, ids):\n",
    "    '''\n",
    "    Convert token IDs back to tokens using the tokenizer\n",
    "    Really just a mapping function for tokenizer.decode()\n",
    "    '''\n",
    "    tokens = tokenizer.decode(ids)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flash \\\\n Other white creatures you control get +1/+1 . \\\\n Other blue creatures you control get +1/+1 .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_tokenizer.decode(tokenize_dataset_dict['validation']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flash \\\\n Other white creatures you control get +1/+1 . \\\\n Other blue creatures you control get +1/+1 .'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_to_tokens(trained_tokenizer, tokenize_dataset_dict['validation']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_dataset_dict['validation']['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set formating to torch of the DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the tokenized dataset is in the correct format for PyTorch\n",
    "\n",
    "# old\n",
    "# tokenize_dataset_dict.set_format(type='torch', columns=['sentence', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])\n",
    "\n",
    "# New\n",
    "tokenize_dataset_dict.set_format(type='torch', columns=['sentence', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Note that these have been padded out to the max_length\n",
    "\n",
    "for i in range(0, 11):\n",
    "    print(tokenize_dataset_dict['train'][i]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11):\n",
    "    print(tokenize_dataset_dict['train'][i]['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose one — \\n • Destroy target Cleric . \\n • Return target Cleric card from your graveyard to your hand . \\n • Target player loses 2 life .\n",
      "( {T} : Add {W} or {B} . )\n",
      "Explosive Welcome deals 5 damage to any target and 3 damage to any other target . Add {R} {R} {R} .\n",
      "Vigilance (Attacking doesn't cause this creature to tap . )\n",
      "Trample \\n Alexios , Deimos of Kosmos attacks each combat if able , can't be sacrificed , and can't attack its owner . \\n At the beginning of each player's upkeep , that player gains control of Alexios , untaps it , and puts a +1/+1 counter on it . It gains haste until end of turn .\n",
      "When Gang of Devils dies , it deals 3 damage divided as you choose among one , two , or three targets .\n",
      "Ninjutsu {2} {U} ( {2} {U} , Return an unblocked attacker you control to hand : Put this card onto the battlefield from your hand tapped and attacking . ) \\n Whenever Mist-Syndicate Naga deals combat damage to a player , create a token that's a copy of Mist-Syndicate Naga .\n",
      "Boros Garrison enters tapped . \\n When Boros Garrison enters , return a land you control to its owner's hand . \\n {T} : Add {R} {W} .\n",
      "Enchant creature \\n Enchanted creature can't attack , block , or transform . \\n Sacrifice another permanent : Attach Bound by Moonsilver to target creature . Activate only as a sorcery and only once each turn .\n",
      "During your turn , creatures you control have hexproof . \\n Each creature you control with toughness greater than its power assigns combat damage equal to its toughness rather than its power .\n",
      "Constellation — Whenever an enchantment you control enters , Setessan Skirmisher gets +1/+1 until end of turn .\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 11):\n",
    "    print(tokenize_dataset_dict['train'][i]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Short Rebuild of the above but with Hugging Face API\n",
    "\n",
    "We might be able to leverage accelerated training by using HF so this could be worth to have.  It also uses a PyTorch backend which I know works on Brandon's system... TensorFlow has been giving me problems in my environment lately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the decoder-only setup\n",
    "from transformers import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(8912, 512)\n",
       "    (wpe): Embedding(500, 512)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x GPT2Block(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=1536, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=512)\n",
       "          (attn_dropout): Dropout(p=0.5, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=2048, nx=512)\n",
       "          (c_proj): Conv1D(nf=512, nx=2048)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=8912, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the model\n",
    "\n",
    "VOCAB_SIZE = 8912 # Define this based on training data vocab size / tokenize size.\n",
    "# TODO: Or does it need to relate to the tokenizer?\n",
    "SEQUENCE_LENGTH = 500 # Define this based on training data sequence length. aka. context window\n",
    "EMBEDDING_DIM = 512 # Configure\n",
    "FEED_FORWARD_DIM = 4 * EMBEDDING_DIM # Standard for decoder-only items.\n",
    "DROPOUT = 0.5 # Configure\n",
    "NUM_HEADS = 16 # Configure attention heads\n",
    "NUM_DECODER_LAYERS = 4 # Configure\n",
    "\n",
    "# Define the model configuration\n",
    "model_config = GPT2Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    n_positions=SEQUENCE_LENGTH,\n",
    "    n_ctx=SEQUENCE_LENGTH,\n",
    "    n_embd=EMBEDDING_DIM,\n",
    "    n_layer=NUM_DECODER_LAYERS,\n",
    "    n_head=NUM_HEADS,\n",
    "    resid_pdrop=DROPOUT,\n",
    "    embd_pdrop=DROPOUT,\n",
    "    attn_pdrop=DROPOUT,\n",
    "    # layer_norm_epsilon=1e-5,\n",
    "    # initializer_range=0.02,\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "gptmodel = GPT2LMHeadModel(model_config)\n",
    "# Output the model layers for inspection\n",
    "gptmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 17429504\n"
     ]
    }
   ],
   "source": [
    "# Check the number of trainable parameters.\n",
    "# How many trainable parameters exist in the gptmodel? (setting p.requires_grad to True for this)\n",
    "trainable_params = sum(p.numel() for p in gptmodel.parameters() if p.requires_grad)\n",
    "print(\"Number of trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight: 4562944 trainable parameters\n",
      "transformer.wpe.weight: 256000 trainable parameters\n",
      "transformer.h.0.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.0.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.0.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.0.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.0.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.0.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.0.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.0.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.0.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.0.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.0.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.0.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.1.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.1.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.1.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.1.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.1.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.1.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.1.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.1.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.1.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.1.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.1.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.1.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.2.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.2.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.2.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.2.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.2.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.2.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.2.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.2.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.2.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.2.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.2.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.2.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.3.ln_1.weight: 512 trainable parameters\n",
      "transformer.h.3.ln_1.bias: 512 trainable parameters\n",
      "transformer.h.3.attn.c_attn.weight: 786432 trainable parameters\n",
      "transformer.h.3.attn.c_attn.bias: 1536 trainable parameters\n",
      "transformer.h.3.attn.c_proj.weight: 262144 trainable parameters\n",
      "transformer.h.3.attn.c_proj.bias: 512 trainable parameters\n",
      "transformer.h.3.ln_2.weight: 512 trainable parameters\n",
      "transformer.h.3.ln_2.bias: 512 trainable parameters\n",
      "transformer.h.3.mlp.c_fc.weight: 1048576 trainable parameters\n",
      "transformer.h.3.mlp.c_fc.bias: 2048 trainable parameters\n",
      "transformer.h.3.mlp.c_proj.weight: 1048576 trainable parameters\n",
      "transformer.h.3.mlp.c_proj.bias: 512 trainable parameters\n",
      "transformer.ln_f.weight: 512 trainable parameters\n",
      "transformer.ln_f.bias: 512 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Print each parameter's name and trainable count individually\n",
    "for name, param in gptmodel.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel()} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference in the reported number of trained parameters between Etienne's custom build and the Hugging Face model.\n",
    "\n",
    "I'm 90% certain it has to do with the positional eoncodings being done slightly different.  48M of his model parameters are from the PosEnc which doesn't show up the same way in the GPT-model.\n",
    "\n",
    "Both models are likely to generate similar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5, # Prior runs show that I am easily over fitting the data at 25 epochs...\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    # logging_steps=10, # This made my loss vs epoch plot too noisy...\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Trainer\n",
    "\n",
    "#mlm is the masked language modeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_fast, mlm=False, return_tensors='pt')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gptmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenize_dataset_dict['train'],\n",
    "    eval_dataset=tokenize_dataset_dict['validation'],\n",
    "    processing_class=tokenizer_fast,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHOO CHOO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7660' max='7660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7660/7660 28:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.195700</td>\n",
       "      <td>3.285212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.079500</td>\n",
       "      <td>2.852345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.758600</td>\n",
       "      <td>2.624456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.592100</td>\n",
       "      <td>2.508041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.510800</td>\n",
       "      <td>2.476660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7660, training_loss=3.0273361006853787, metrics={'train_runtime': 1713.5357, 'train_samples_per_second': 71.484, 'train_steps_per_second': 4.47, 'total_flos': 2372609271398400.0, 'train_loss': 3.0273361006853787, 'epoch': 5.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the Trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the model\n",
    "trainer.save_model(\"../models/hf_gpt2_style\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.90\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll likely need to create our prompt input type thing down here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, num_return_sequences=1, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text using the trained GPT2 model.\n",
    "\n",
    "    Args:\n",
    "        model: The trained GPT2 model\n",
    "        tokenizer: The tokenizer used for encoding/decoding text\n",
    "        prompt: The input prompt text to generate from\n",
    "        max_length: Maximum length of generated sequence\n",
    "        num_return_sequences: Number of sequences to generate\n",
    "        temperature: Controls randomness (higher = more random)\n",
    "\n",
    "    Returns:\n",
    "        List of generated text sequences\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Encode the input prompt\n",
    "    encoded_prompt = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=encoded_prompt['input_ids'],\n",
    "            attention_mask=encoded_prompt['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "        )\n",
    "\n",
    "    # Decode and return the generated sequences\n",
    "    generated_sequences = []\n",
    "    for generated_sequence in output_sequences:\n",
    "        generated_text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        generated_sequences.append(generated_text)\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['llanowar elves-or blocks a creature an opponent controls enters with three +1/+1 counter on it were {T} : Exile a creature of your hand : Create a 2/1 white colorless1 white Spirit creature token . \\\\n Whenever an artifact creature you control attacks , draw a card , then put into the top card of each1/+1 counter on it counter on it and you may put another Treasure token that many +1 counter on it . (To create a +1']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(gptmodel, tokenizer_fast, \"llanowar elves\", max_length=100, num_return_sequences=1, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Advanced Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llanowar elves enters tapped . \\n {T} : Add {U} . It gets +1/+1 until end of turn , Sacrifice an opponent sacrifices a creature : Return target creature card from your graveyard to its owner's hand . If you do , draw a card . You may put a +2/+0 counter on it . Activate only as a land . (If you control an instant or more +X/+X counter from a sorcery card in a random order . ) \\\n"
     ]
    }
   ],
   "source": [
    "# This has some nucleus sampling and top-k sampling\n",
    "# This is likely for us a better way to generate text\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "# Load  trained model and tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"./model/path\")\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"./tokenizer/path\")\n",
    "\n",
    "# Prepare your prompt\n",
    "prompt = \"llanowar elves\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs = tokenizer_fast(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "\n",
    "# TODO: change `gptmodel` to `model` and `tokenizer_fast` to `tokenizer`\n",
    "# Generate text\n",
    "outputs = gptmodel.generate(\n",
    "    inputs.input_ids.to(device),\n",
    "    max_length=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer_fast.pad_token_id,\n",
    "    eos_token_id=tokenizer_fast.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer_fast.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcg_generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
